{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxZreWqOdMcBe6eRs2cTIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-T00/NLP_Codes/blob/example/Transformers_SentimentAnalysisWithPipeline_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfKsJcpbPQUN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "ModelName = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "pt_model = AutoModelForSequenceClassification.from_pretrained(ModelName)"
      ],
      "metadata": {
        "id": "6T-GTSHAPftW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(ModelName)"
      ],
      "metadata": {
        "id": "x1Y-Wf8MQxz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SentPos = 'I love NLP'\n",
        "SentNeg = 'I dont love NLP'"
      ],
      "metadata": {
        "id": "AATsLco1vchj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(SentPos)"
      ],
      "metadata": {
        "id": "-8mJDJd7eTCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#101 means begening of sequence , 102 means end of sequence and 0 means padding\n",
        "input"
      ],
      "metadata": {
        "id": "eRRLKaymgcBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: If you want to know what this token ID mean, you can see and use the following code with example :\n",
        "tokenizer.convert_ids_to_tokens([2293])"
      ],
      "metadata": {
        "id": "X23e67d8ib5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_batch = tokenizer(\n",
        "    [SentPos, SentNeg],\n",
        "    padding = True,\n",
        "    truncation = True,\n",
        "    max_length = 512,\n",
        "    return_tensors = 'pt'\n",
        ")"
      ],
      "metadata": {
        "id": "5LW92qdclLGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_batch"
      ],
      "metadata": {
        "id": "bpzApNF3krNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_outputs = pt_model(**pt_batch)"
      ],
      "metadata": {
        "id": "0JNrHjrGwmFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "pt_prediction = nn.functional.softmax(pt_outputs.logits, dim = -1)"
      ],
      "metadata": {
        "id": "x3UNnqPrw9Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_prediction"
      ],
      "metadata": {
        "id": "OvrfBPY3x6vJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}